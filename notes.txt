AIra/
│
├── aira/
│   ├── main.py              # FastAPI entry
│   ├── api/
│   │   ├── chat.py
│   │   ├── rag.py
│   │   └── agent.py
│   │
│   ├── core/
│   │   ├── llm_loader.py
│   │   ├── prompt_manager.py
│   │   └── callbacks.py
│   │
│   ├── chains/
│   │   ├── basic_chain.py
│   │   └── rag_chain.py
│   │
│   ├── agents/
│   │   ├── base_agent.py
│   │   └── tool_agent.py
│   │
│   ├── tools/
│   │   ├── search.py
│   │   └── calculator.py
│   │
│   └── memory/
│       └── conversation.py
│
├── training/
│   ├── lora_finetune.py
│   ├── qlora_finetune.py
│   └── configs/
│
├── data/
│   ├── raw/
│   ├── processed/
│   └── dvc.yaml
│
├── inference/
│   ├── vllm_server.py
│   └── client.py
│
├── mlflow/
│
├── docker/
│   ├── Dockerfile
│   └── docker-compose.yml
│
├── .github/
│   └── workflows/
│       └── ci.yml
│
├── README.md
├── requirements.txt
└── pyproject.toml


##### STEP-1 ######

* we use __init__.py file --> code in it run first as soon as the package is imported
* from pathlib import Path --> Imports a modern way to handle filesystem paths (better than strings).
* BASE_DIR = Path(__file__).parent.parent --> goes two levels back from the current script
* MODEL_DIR.mkdir(exist_ok=True, parents=True) --> parents=True makes sure the the parents dir exist
* What do you mean by prompt abstraction?
  In AIra, prompts are treated as first-class components—defined independently from model 
  inference and orchestrated via LangChain PromptTemplates—so they can evolve with RAG, 
  agents, and fine-tuning without changing core logic


our current position(sha id: b1d656d-->we are getting hellusinated and unstructured outputs from 
                    the model)
    Raw LLM generation
    ↓
    Unbounded output
    ↓
    Hard to control
    ↓
    Not production-safe

    1. we are using Qwen/Qwen3-0.6B which is a text generation model
    2. raw inference, model don't know when to stop
    Raw inference ≠ Production inference
