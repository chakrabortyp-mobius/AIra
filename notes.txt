AIra/
‚îÇ
‚îú‚îÄ‚îÄ aira/
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # FastAPI entry
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rag.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ agent.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm_loader.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prompt_manager.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ callbacks.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ chains/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ basic_chain.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rag_chain.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_agent.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tool_agent.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ tools/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ search.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ calculator.py
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ memory/
‚îÇ       ‚îî‚îÄ‚îÄ conversation.py
‚îÇ
‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îú‚îÄ‚îÄ lora_finetune.py
‚îÇ   ‚îú‚îÄ‚îÄ qlora_finetune.py
‚îÇ   ‚îî‚îÄ‚îÄ configs/
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îú‚îÄ‚îÄ processed/
‚îÇ   ‚îî‚îÄ‚îÄ dvc.yaml
‚îÇ
‚îú‚îÄ‚îÄ inference/
‚îÇ   ‚îú‚îÄ‚îÄ vllm_server.py
‚îÇ   ‚îî‚îÄ‚îÄ client.py
‚îÇ
‚îú‚îÄ‚îÄ mlflow/
‚îÇ
‚îú‚îÄ‚îÄ docker/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.yml
‚îÇ
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îî‚îÄ‚îÄ ci.yml
‚îÇ
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ pyproject.toml

=========================================================
Layer	                Responsibility                  |
------                 ----------------                 |
LLM	                    Generate text                   |
Chain	                How to answer                   |
RAG	                    Retrieve + answer               |
Agent	                Decide which chain to use       |
=========================================================

- [x] Week 1: Core LLM Inference & Modular Architecture 
- [ ] Week 2: FastAPI-based Serving Layer(main.py should not call the llm directly)
   üìå What We Are NOT Doing in Week-2
    ‚ùå Auth
    ‚ùå RAG
    ‚ùå Memory
    ‚ùå Agents
    ‚ùå MLflow
    Version	Meaning
    0.1.0	Core LLM inference (Week 1)
    0.2.0	API serving layer (Week 2)
- [ ] Week 3: RAG with Vector Stores + DVC 
- [ ] Week 4: Agentic AI with Tools & Memory 
- [ ] Week 5: LoRA Fine-Tuning + MLflow 
- [ ] Week 6: QLoRA + Optimization 
- [ ] Week 7: vLLM Deployment 
- [ ] Week 8: Docker + CI/CD 
- [ ] Week 9+: Cloud Deployment & Monitoring


##### STEP-1 ######

* we use __init__.py file --> code in it run first as soon as the package is imported
* from pathlib import Path --> Imports a modern way to handle filesystem paths (better than strings).
* BASE_DIR = Path(__file__).parent.parent --> goes two levels back from the current script
* MODEL_DIR.mkdir(exist_ok=True, parents=True) --> parents=True makes sure the the parents dir exist
* we use @staticmethod inside class for any method in the class so that when we can use the 
  method directly withiout making any object from the class. it does not take self as a inut inside
  class, so it cannot interact with other methods or attributes inside class.
  e.g.--> prompt = PromptManager.get_basic_chat_prompt()
* What do you mean by prompt abstraction?
  In AIra, prompts are treated as first-class components‚Äîdefined independently from model 
  inference and orchestrated via LangChain PromptTemplates‚Äîso they can evolve with RAG, 
  agents, and fine-tuning without changing core logic


our current position(sha id: b1d656d-->we are getting hellusinated and unstructured outputs from 
                    the model)
    Raw LLM generation
    ‚Üì
    Unbounded output
    ‚Üì
    Hard to control
    ‚Üì
    Not production-safe

    1. we are using Qwen/Qwen3-0.6B which is a text generation model
    2. raw inference, model don't know when to stop
    Raw inference ‚â† Production inference


our current position(sha id: d67d6a0-->we are getting proper output with <think> as we are using
                    the template on which the qwen was trained on)
    Tokenizer-aware LLM abstraction
        ‚Üì
    Qwen-native chat prompt alignment
        ‚Üì
    LangChain BasicChain orchestration
        ‚Üì
    EOS-driven bounded generation
        ‚Üì
    Clean, deterministic responses




 
    *currently our model is CLI(commad line interface) only
    * dependencies.py (ask why we need this)
        Later you can:
            Cache the model
            Swap models
            Add RAG chains
            Mock in tests


##### STEP-3 ######
    RAG plan
        Hybrid Search                       Week 4
        Re-ranking                          Week 4
        Query Decomposition                 Week 5
        HyDE                                Week 5
        Parent Retrieval                    Week 3
        Self-Querying                       Week 6

    *in rag we keep the model for emmbeding model to be small unlike the main llm model
    üîú Next step (choose one ‚Äî ONE at a time) 
    1Ô∏è‚É£ PDF ingestion pipeline (PyPDFLoader ‚Üí chunk ‚Üí FAISS) 
    2Ô∏è‚É£ BM25 + Hybrid Search 
    3Ô∏è‚É£ DVC setup for RAG datasets 
    4Ô∏è‚É£ RAG evaluation hooks

